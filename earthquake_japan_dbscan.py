# -*- coding: utf-8 -*-
"""earthquake_japan_dbscan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DpTvwn4JNrDE_8GjX7HzvRd3VmTWqkp4

<h1 align=center><font size = 5>Clustering Significant Earthquakes in Japan with DBSCAN</font></h1>

<br>

<img src="https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iUycfdaMMr1k/v1/-1x-1.jpg" width=1000 height=600 alt="https://www.bloomberg.com/">

<small>Picture Source: <a href="https://www.bloomberg.com/">bloomberg</a>

<br>

<h2>Data Description</h2>

<br>

<h3>Context</h3>

<p>The National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies, scientists, critical facilities, and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information.</p>

<br>

<h3>Content</h3>

<p>This dataset includes a record of the date, time, location, depth, magnitude, and source of every earthquake with a reported <b>magnitude 5.5 or higher since 1965</b>.</p>

<br>

<h3>Dataset Link</h3>

You can download or take a look at original website of the dataset: [Kaggle](https://www.kaggle.com/datasets/usgs/earthquake-database)

<br>

<h3>License</h3>
<p>CC0: Public Domain</p>

<br>

<h3>Keywords</h3>
<ul>
  <li>Geology</li>
  <li>Earth Science</li>
  <li>Earthquake</li>
  <li>Japan</li>
</ul>

<br>

<h3>License</h3>    
<p>CC BY-SA 4.0</p>

<br>
    
<h3>Sources</h3>
<ul>
    <li><a href="https://www.kaggle.com/datasets/usgs/earthquake-database">Kaggle</a></li>
    <li><a href="https://www.bloomberg.com/news/articles/2019-06-18/magnitude-6-8-quake-hits-off-japan-tsunami-advisory-issued">Bloomberg</a></li>
</ul>

<hr>

<h2>Objective for This Notebook</h2>

Within the scope of this project, a clustering task was done. With density based clustering, we were trying to classify or make cluster of points (based of their location, earthquake depth and magnitude).

<div class="alert alert-block alert-info" style="margin-top: 20px">
<li><a href="https://#item11">Importing Libraries</a></li>
<li><a href="https://#item12">Load the Dataset</a></li>
<li><a href="https://#item13">Visualization</a></li>

<br>

<p>Estimated Time Needed: <strong>25 min</strong></p>

</div>

<hr>

<a id='item11'></a>

<h2>Importing Libraries</h2>
"""

import numpy as np 
from sklearn.cluster import DBSCAN 
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler 
import matplotlib.pyplot as plt
import csv
import pandas as pd
import numpy as np

"""<a id='item12'></a>

<h2>Load the Dataset</h2>


You can download the data from [Kaggle](https://www.kaggle.com/datasets/vinayakshanawad/weedcrop-image-dataset). After downloading and importing process, we can unzip.
"""

!unzip archive.zip

filename='database.csv'

"""<p>Let's take a look at our dataset</p>"""

pdf = pd.read_csv(filename)

pdf.dropna(subset=['Latitude', 'Longitude', 'Depth', 'Magnitude', 'Source'])

pdf.head(10)

pdf.tail(10)

pdf.info()

"""<a id='item13'></a>

<h2>Visualization</h2>

<p>The matplotlib basemap toolkit is a library for plotting 2D data on maps in Python. Basemap does not do any plotting on itâ€™s own, but provides the facilities to transform coordinates to a map projections.</p>
"""

!pip3 install basemap

from mpl_toolkits.basemap import Basemap

import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

"""<p>Approximate coordinates of Japan</p>"""

llon=122.77
ulon=153.14
llat=23.03
ulat=50.54

print(f"llon: {llon}")
print(f"ulon: {ulon}")
print(f"llat: {llat}")
print(f"ulat: {ulat}")

pdf = pdf[(pdf['Longitude'] > llon) & (pdf['Longitude'] < ulon) & (pdf['Latitude'] > llat) &(pdf['Latitude'] < ulat)]

my_map = Basemap(projection='merc',
            resolution = 'l', area_thresh = 1000.0,
            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)
            urcrnrlon=ulon, urcrnrlat=ulat) #max longitude (urcrnrlon) and latitude (urcrnrlat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()

xs,ys = my_map(np.asarray(pdf.Longitude), np.asarray(pdf.Latitude))
pdf['xm']= xs.tolist()
pdf['ym'] =ys.tolist()

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()
for index, row in pdf.iterrows():
   my_map.plot(row.xm, row.ym, markerfacecolor =([1, 0, 0]),  marker='o', markersize= 5, alpha = 0.75)
plt.show()

"""<h3>Clustering of Stations Based on Their Location i.e. Lat & Lon</h3>

<p><i>DBSCAN</i> form sklearn library can run <i>DBSCAN</i> clustering from vector array or distance matrix. In our case, we pass it the Numpy array Clus_dataSet to find core samples of high density and expands clusters from them.</p>
"""

from sklearn.cluster import DBSCAN
import sklearn.utils
from sklearn.preprocessing import StandardScaler

sklearn.utils.check_random_state(1000)
Clus_dataSet = pdf[['xm', 'ym']]
Clus_dataSet = np.nan_to_num(Clus_dataSet)
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

"""Computing DBSCAN."""

db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
pdf["Clus_Db"]=labels

labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels))

"""A sample of clusters:"""

pdf[["Source", "Depth", "Clus_Db"]].head(5)

set(labels)

"""<h3>Visualization of Clusters Based on Location</h3>"""

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

my_map = Basemap(projection='merc',
            resolution = 'l', area_thresh = 1000.0,
            llcrnrlon=llon, llcrnrlat=llat, #min longitude (llcrnrlon) and latitude (llcrnrlat)
            urcrnrlon=ulon, urcrnrlat=ulat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()

colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for clust_number in set(labels):
    c=(([0.4, 0.4, 0.4]) if clust_number == -1 else colors[np.int(clust_number)])
    clust_set = pdf[pdf.Clus_Db == clust_number]                    
    my_map.scatter(clust_set.xm, clust_set.ym, color=c,  marker='o', s=10, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set.xm) 
        ceny=np.mean(clust_set.ym) 
        plt.text(cenx, ceny, str(clust_number), fontsize=18, color='red',)
        print ("Cluster "+str(clust_number)+', Avg Depth: '+ str(np.mean(clust_set.Depth)))

"""<h3>Clustering of Sources Based on Their Location i.e. Lat & Lon and Depth</h3>"""

from sklearn.cluster import DBSCAN
import sklearn.utils
from sklearn.preprocessing import StandardScaler

sklearn.utils.check_random_state(1000)
Clus_dataSet = pdf[['xm', 'ym', 'Depth']]
Clus_dataSet = np.nan_to_num(Clus_dataSet)
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
pdf["Clus_Db"]=labels

labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels))

pdf[["Source", "Depth", "Clus_Db"]].head(5)

"""<h3>Visualization of Clusters Based on Location and Depth</h3>"""

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

my_map = Basemap(projection='merc',
            resolution = 'l', area_thresh = 1000.0,
            llcrnrlon=llon, llcrnrlat=llat,
            urcrnrlon=ulon, urcrnrlat=ulat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()

colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for clust_number in set(labels):
    c=(([0.4, 0.4, 0.4]) if clust_number == -1 else colors[np.int(clust_number)])
    clust_set = pdf[pdf.Clus_Db == clust_number]                    
    my_map.scatter(clust_set.xm, clust_set.ym, color=c,  marker='o', s=10, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set.xm) 
        ceny=np.mean(clust_set.ym) 
        plt.text(cenx, ceny, str(clust_number), fontsize=18, color='red')
        print ("Cluster " + str(clust_number)+', Avg Depth: '+ str(np.mean(clust_set.Depth)))

"""<h3>Visualization of Clusters Based on Location and Magnitude</h3>"""

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

sklearn.utils.check_random_state(1000)
Clus_dataSet = pdf[['xm', 'ym', 'Magnitude']]
Clus_dataSet = np.nan_to_num(Clus_dataSet)
Clus_dataSet = StandardScaler().fit_transform(Clus_dataSet)

db = DBSCAN(eps=0.15, min_samples=10).fit(Clus_dataSet)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
pdf["Clus_Db"]=labels

labels

realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)
clusterNum = len(set(labels))

pdf[["Source", "Magnitude", "Clus_Db"]].head(5)

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
from pylab import rcParams
# %matplotlib inline
rcParams['figure.figsize'] = (20, 20)

my_map = Basemap(projection='merc',
            resolution = 'l', area_thresh = 1000.0,
            llcrnrlon=llon, llcrnrlat=llat,
            urcrnrlon=ulon, urcrnrlat=ulat)

my_map.drawcoastlines()
my_map.drawcountries()
my_map.fillcontinents(color = 'white', alpha = 0.3)
my_map.shadedrelief()

colors = plt.get_cmap('jet')(np.linspace(0.0, 1.0, clusterNum))

for clust_number in set(labels):
    c=(([0.4, 0.4, 0.4]) if clust_number == -1 else colors[np.int(clust_number)])
    clust_set = pdf[pdf.Clus_Db == clust_number]                    
    my_map.scatter(clust_set.xm, clust_set.ym, color=c,  marker='o', s=10, alpha = 0.85)
    if clust_number != -1:
        cenx=np.mean(clust_set.xm) 
        ceny=np.mean(clust_set.ym) 
        plt.text(cenx, ceny, str(clust_number), fontsize=18, color='red')
        print ("Cluster " + str(clust_number)+', Avg Magnitude: '+ str(np.mean(clust_set.Magnitude)))

"""<hr>

<h1>Contact Me<h1>

<p>If you have something to say to me please contact me:<p>

*   Twitter: https://twitter.com/Doguilmak
*   Mail address: doguilmak@gmail.com
"""